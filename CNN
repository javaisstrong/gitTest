import numpy as np
import matplotlib.pyplot as plt
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# --- ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”© ---
transform = transforms.Compose([transforms.ToTensor()])
mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(mnist_train, batch_size=1, shuffle=True)

# --- CNN êµ¬ì„±ìš”ì†Œ êµ¬í˜„ ---
def conv2d(y, filter):
    HSize, WSize = y.shape
    HF, WF = filter.shape
    HMove = HSize - HF + 1
    WMove = WSize - WF + 1
    output = np.zeros((HMove, WMove))   
    regions = []  # ë‚˜ì¤‘ì— ì—­ì „íŒŒìš©

    for i in range(HMove):
        for j in range(WMove):
            yRegion = y[i:i+HF, j:j+WF]
            output[i, j] = np.sum(filter * yRegion)
            regions.append(((i, j), yRegion))

    return output,regions

def ReLu(y):
    return np.maximum(0, y)

def maxpooling(y, size):
    HS, WS = y.shape
    HNumber = HS // size
    WNumber = WS // size
    yPart = np.zeros((HNumber, WNumber))
    for i in range(HNumber):
        for j in range(WNumber):
            yPool = y[i*size:i*size+size, j*size:j*size+size]
            yPart[i, j] = np.max(yPool)
    return yPart

def flat(y, W, b):
    flattened = y.flatten()
    return np.dot(W, flattened) + b

def softmax(x):
    e = np.exp(x - np.max(x))
    return e / np.sum(e)

def cross_entropy_loss(pred_probs, true_class_index):
    return -np.log(pred_probs[true_class_index] + 1e-8)  # ì•ˆì •ì„± ìœ„í•´ Îµ ì¶”ê°€

def backPropagation(y, true_class_index, flattened, w, b, conv_regions,filter,lr=0.01):
    onehot = np.zeros_like(y)
    onehot[true_class_index] = 1
    
    delta = y - onehot
    dW = np.outer(delta, flattened)
    db = delta

    w -= lr * dW
    b -= lr * db

    dFilter = np.zeros_like(filter)
    for (i, j), region in conv_regions:
        for k in range(len(delta)):
            dFilter += delta[k] * np.mean(region) * 0.01  # ê°„ë‹¨í•œ ì „íŒŒ
    filter -= lr * dFilter
    return w, b, filter


################################################################################################################################################
################################################################################################################################################
# --- ëª¨ë¸ êµ¬ì¡° ---
print("ë™ì‘ ì‹œì‘")

filter = np.array([[2, 2], [2, -4]], dtype=np.float64)

sample_input, _ = next(iter(train_loader))
sample_input = sample_input.squeeze().numpy() * 255
pr1,regions = conv2d(sample_input, filter)
pr2 = ReLu(pr1)
pr3 = maxpooling(pr2, 2)
flattened = pr3.flatten()
n = len(flattened)
Nneuron = 10  # MNIST í´ë˜ìŠ¤ëŠ” 0~9

w = np.random.randn(Nneuron, n) * np.sqrt(2/n)
b = np.zeros(Nneuron)

# --- í›ˆë ¨ ì‹œì‘ ---
print("í›ˆë ¨ì‹œì‘")

loss_list = []
for epoch in range(100):
    total_loss = 0
    print("for ë¬¸ ì‹œì‘")

    for i, (img, label) in enumerate(train_loader):
        if i >= 100:  # 100ê°œë§Œ í•™ìŠµí•˜ê³  ë‹¤ìŒ epochìœ¼ë¡œ
            print(f"â¹ ì´ë¯¸ì§€ 100ê°œ í•™ìŠµ ì™„ë£Œ (Epoch {epoch+1})")
            break
        x = img.squeeze().numpy() * 255
        true_class = label.item()

        pr1,regions = conv2d(x, filter)
        pr2 = ReLu(pr1)
        pr3 = maxpooling(pr2, 2)
        flattened = pr3.flatten()

        pr4 = flat(pr3, w, b)
        pr5 = softmax(pr4)
        loss = cross_entropy_loss(pr5, true_class)
        total_loss += loss

        w, b, filter = backPropagation(pr5, true_class, flattened, w, b, regions,filter, lr=0.01)

    avg_loss = total_loss / 100
    loss_list.append(avg_loss)
    print(f"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}")
    # â›”ï¸ ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
    if avg_loss < 0.2:
        print(f"ğŸ‰ Lossê°€ 0.2 ì´í•˜ë¡œ ë„ë‹¬í•˜ì—¬ ì¡°ê¸° ì¢…ë£Œ (Epoch {epoch+1})")
        break
    
# --- ê·¸ë˜í”„ ì¶œë ¥ ---
plt.plot(loss_list, marker='o')
plt.title("Average Loss per Epoch (on 100 samples)")
plt.xlabel("Epoch")
plt.ylabel("Cross Entropy Loss")
plt.grid(True)
plt.show()



# --- í•™ìŠµ ì™„ë£Œ í›„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ í™•ì¸ ---
test_samples = []
for i, (img, label) in enumerate(train_loader):
    if i >= 5:
        break
    x = img.squeeze().numpy() * 255
    true_class = label.item()

    pr1, _ = conv2d(x, filter)
    pr2 = ReLu(pr1)
    pr3 = maxpooling(pr2, 2)
    pr4 = flat(pr3, w, b)
    pr5 = softmax(pr4)
    predicted_class = np.argmax(pr5)

    test_samples.append((x, true_class, predicted_class))

# --- ì‹œê°í™” ---
plt.figure(figsize=(10, 3))
for i, (img, true_cls, pred_cls) in enumerate(test_samples):
    plt.subplot(1, 5, i+1)
    plt.imshow(img, cmap='gray')
    plt.title(f"T:{true_cls}, P:{pred_cls}")
    plt.axis('off')
plt.suptitle("Test Samples - True vs Predicted")
plt.tight_layout()
plt.show()