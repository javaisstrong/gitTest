import numpy as np
import matplotlib.pyplot as plt
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# --- 데이터 전처리 및 로딩 ---
transform = transforms.Compose([transforms.ToTensor()])
mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(mnist_train, batch_size=1, shuffle=True)

# --- CNN 구성요소 구현 ---
def conv2d(y, filter):
    HSize, WSize = y.shape
    HF, WF = filter.shape
    HMove = HSize - HF + 1
    WMove = WSize - WF + 1
    output = np.zeros((HMove, WMove))   
    regions = []  # 나중에 역전파용

    for i in range(HMove):
        for j in range(WMove):
            yRegion = y[i:i+HF, j:j+WF]
            output[i, j] = np.sum(filter * yRegion)
            regions.append(((i, j), yRegion))

    return output,regions

def ReLu(y):
    return np.maximum(0, y)

def maxpooling(y, size):
    HS, WS = y.shape
    HNumber = HS // size
    WNumber = WS // size
    yPart = np.zeros((HNumber, WNumber))
    for i in range(HNumber):
        for j in range(WNumber):
            yPool = y[i*size:i*size+size, j*size:j*size+size]
            yPart[i, j] = np.max(yPool)
    return yPart

def flat(y, W, b):
    flattened = y.flatten()
    return np.dot(W, flattened) + b

def softmax(x):
    e = np.exp(x - np.max(x))
    return e / np.sum(e)

def cross_entropy_loss(pred_probs, true_class_index):
    return -np.log(pred_probs[true_class_index] + 1e-8)  # 안정성 위해 ε 추가

def backPropagation(y, true_class_index, flattened, w, b, conv_regions,filter,lr=0.01):
    onehot = np.zeros_like(y)
    onehot[true_class_index] = 1
    
    delta = y - onehot
    dW = np.outer(delta, flattened)
    db = delta

    w -= lr * dW
    b -= lr * db

    dFilter = np.zeros_like(filter)
    for (i, j), region in conv_regions:
        for k in range(len(delta)):
            dFilter += delta[k] * np.mean(region) * 0.01  # 간단한 전파
    filter -= lr * dFilter
    return w, b, filter


################################################################################################################################################
################################################################################################################################################
# --- 모델 구조 ---
print("동작 시작")

filter = np.array([[2, 2], [2, -4]], dtype=np.float64)

sample_input, _ = next(iter(train_loader))
sample_input = sample_input.squeeze().numpy() * 255
pr1,regions = conv2d(sample_input, filter)
pr2 = ReLu(pr1)
pr3 = maxpooling(pr2, 2)
flattened = pr3.flatten()
n = len(flattened)
Nneuron = 10  # MNIST 클래스는 0~9

w = np.random.randn(Nneuron, n) * np.sqrt(2/n)
b = np.zeros(Nneuron)

# --- 훈련 시작 ---
print("훈련시작")

loss_list = []
for epoch in range(100):
    total_loss = 0
    print("for 문 시작")

    for i, (img, label) in enumerate(train_loader):
        if i >= 100:  # 100개만 학습하고 다음 epoch으로
            print(f"⏹ 이미지 100개 학습 완료 (Epoch {epoch+1})")
            break
        x = img.squeeze().numpy() * 255
        true_class = label.item()

        pr1,regions = conv2d(x, filter)
        pr2 = ReLu(pr1)
        pr3 = maxpooling(pr2, 2)
        flattened = pr3.flatten()

        pr4 = flat(pr3, w, b)
        pr5 = softmax(pr4)
        loss = cross_entropy_loss(pr5, true_class)
        total_loss += loss

        w, b, filter = backPropagation(pr5, true_class, flattened, w, b, regions,filter, lr=0.01)

    avg_loss = total_loss / 100
    loss_list.append(avg_loss)
    print(f"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}")
    # ⛔️ 조기 종료 조건
    if avg_loss < 0.2:
        print(f"🎉 Loss가 0.2 이하로 도달하여 조기 종료 (Epoch {epoch+1})")
        break
    
# --- 그래프 출력 ---
plt.plot(loss_list, marker='o')
plt.title("Average Loss per Epoch (on 100 samples)")
plt.xlabel("Epoch")
plt.ylabel("Cross Entropy Loss")
plt.grid(True)
plt.show()



# --- 학습 완료 후 테스트 샘플 확인 ---
test_samples = []
for i, (img, label) in enumerate(train_loader):
    if i >= 5:
        break
    x = img.squeeze().numpy() * 255
    true_class = label.item()

    pr1, _ = conv2d(x, filter)
    pr2 = ReLu(pr1)
    pr3 = maxpooling(pr2, 2)
    pr4 = flat(pr3, w, b)
    pr5 = softmax(pr4)
    predicted_class = np.argmax(pr5)

    test_samples.append((x, true_class, predicted_class))

# --- 시각화 ---
plt.figure(figsize=(10, 3))
for i, (img, true_cls, pred_cls) in enumerate(test_samples):
    plt.subplot(1, 5, i+1)
    plt.imshow(img, cmap='gray')
    plt.title(f"T:{true_cls}, P:{pred_cls}")
    plt.axis('off')
plt.suptitle("Test Samples - True vs Predicted")
plt.tight_layout()
plt.show()